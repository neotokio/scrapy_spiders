# Scrapy Spiders Examples

Repositories with Scrapy made web crawlers.

<b>What's inside?</b>

Scrapers use different approach.

*Scraping HTML rendered content using absolute paths
*Scraping HTML rendered content using relative paths (xpath logic selection)
*Scraping JavaScript rendered content using Scrapy-Splash
*Scraping content from local file
*Additional scripts for processing data (Extracting URLS from local HTML File, processing URLS from JSON API)
*JSON pipelines

<b>Usability</b>

All code was written for private use, therefore it's most likley unusable for you. Ie. sometimes we first downloaded all URLs to local file using JSON API, because it was easier than crearting sendForm request in Javascript for parsing each page, however, all files 'needed for work' are in each repository. You will need to make certain tweaks for them to work.
<p></p>
Therefore, you should use this repositories mostly as reference when stuck on writing your scrapy logic/xpath logic or when you simply look for some alternative approach.
<p></p>
